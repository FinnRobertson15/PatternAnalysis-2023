{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FinnRobertson15/PatternAnalysis-2023/blob/topic-recognition/Colab%20version.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yzCmT9TUJY10",
        "outputId": "997681ae-a0d2-448e-d290-b00a8dc22666"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\", force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Lb7CGdEJCQg",
        "outputId": "a693cf6b-0d28-474e-ed69-f7192a4700f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/AD_NC\n"
          ]
        }
      ],
      "source": [
        "%cd /content/drive/MyDrive/AD_NC"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mgx67Xe5dsgs",
        "outputId": "b7850ff6-2922-444d-ee4f-48498dcaeacf"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NC.zip  \u001b[0m\u001b[01;34mtest\u001b[0m/  \u001b[01;34mtrain\u001b[0m/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_eAax1mMJpyw",
        "outputId": "daf77648-1909-41f9-c05f-0be751ec7d6d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "unzip:  cannot find or open NC.zip, NC.zip.zip or NC.zip.ZIP.\n"
          ]
        }
      ],
      "source": [
        "!unzip NC.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "_8PBaJLSJSCP"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as transforms\n",
        "import time\n",
        "import numpy as np\n",
        "# import pandas as pd\n",
        "import math\n",
        "import os\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.utils import shuffle\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.datasets import ImageFolder\n",
        "import os\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader, TensorDataset, ConcatDataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "7zvKyWj2J7Yk"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, root_dir, transform=None):\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "        self.image_paths = os.listdir(root_dir)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = os.path.join(self.root_dir, self.image_paths[idx])\n",
        "        image = Image.open(img_name)\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image\n",
        "\n",
        "class SiameseDataset(Dataset):\n",
        "    def __init__(self, AD, NC):\n",
        "        self.AD = AD\n",
        "        self.NC = NC\n",
        "        self.labels = torch.cat((torch.ones(len(AD)), torch.zeros(len(NC))), dim=0)\n",
        "\n",
        "    def __len__(self):\n",
        "        return min(len(self.AD), len(self.NC))\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img1 = self.AD[idx]\n",
        "        label1 = self.labels[idx]\n",
        "        img2 = self.NC[idx]\n",
        "        label2 = self.labels[len(self.AD) + idx]\n",
        "\n",
        "        return img1, img2, label1, label2\n",
        "\n",
        "# Set the device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "\n",
        "size = 256\n",
        "transform_X = transforms.Compose([\n",
        "    transforms.Resize((size, size)),\n",
        "    transforms.ToTensor(),\n",
        "    # transforms.Lambda(lambda x: x / 255.0),\n",
        "    # transforms.Normalize(mean=[0.5], std=[0.5])  # Adjust according to your data statistics\n",
        "])\n",
        "\n",
        "\n",
        "# Replace 'your_nii_folder' with the path to your folder containing .nii files\n",
        "# nii_dataset = NiiDataset(root_dir=r'keras_png_slices_data\\keras_png_slices_data\\keras_png_slices_train')\n",
        "loaders = {}\n",
        "\n",
        "for stage in ['test']:\n",
        "    loaders[stage] = {}\n",
        "    AD = CustomDataset(root_dir=os.path.join(stage, 'AD'), transform=transform_X)\n",
        "    NC = CustomDataset(root_dir=os.path.join(stage, 'NC'), transform=transform_X)\n",
        "    loaders[stage] = SiameseDataset(AD, NC)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for load in loaders.values():\n",
        "  print(len(load))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oW2JaTc0dXLz",
        "outputId": "758fa1b1-dbfb-48eb-d982-03b676d628a4"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "4460\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "puuj1CSURmJ9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "532995b4-c399-4666-961a-5513ebf7de7b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "\n",
        "class SiameseNetwork(nn.Module):\n",
        "    def __init__(self, pretrained=True):\n",
        "        super(SiameseNetwork, self).__init__()\n",
        "        self.resnet = models.resnet18(pretrained=pretrained)\n",
        "        # Modify the first convolution layer to accept single-channel input\n",
        "        self.resnet.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "        self.linear = nn.Linear(1000, 2)  # 1000 is the number of output features from ResNet\n",
        "\n",
        "    def forward(self, x1, x2):\n",
        "        output1 = self.resnet(x1)\n",
        "        output2 = self.resnet(x2)\n",
        "        return self.linear(torch.abs(output1 - output2))\n",
        "\n",
        "model = SiameseNetwork()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "ocj6rcu7Rn4-"
      },
      "outputs": [],
      "source": [
        "class ContrastiveLoss(torch.nn.Module):\n",
        "    def __init__(self, margin=2.0):\n",
        "        super(ContrastiveLoss, self).__init__()\n",
        "        self.margin = margin\n",
        "\n",
        "    def forward(self, output1, output2, label):\n",
        "        euclidean_distance = F.pairwise_distance(output1, output2)\n",
        "        loss_contrastive = torch.mean((1 - label) * torch.pow(euclidean_distance, 2) +\n",
        "                                      label * torch.pow(torch.clamp(self.margin - euclidean_distance, min=0.0), 2))\n",
        "        return loss_contrastive"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the network, loss function, and optimizer\n",
        "siamese_net = SiameseNetwork(pretrained=True).to(device)\n",
        "criterion = ContrastiveLoss()\n",
        "optimizer = optim.Adam(siamese_net.parameters(), lr=0.0005)\n",
        "\n",
        "# Training loop\n",
        "epochs = 2\n",
        "total_step = len(loaders['test'])\n",
        "for epoch in range(epochs):\n",
        "  for i, (i1, i2, l1, l2) in enumerate(loaders['test']):\n",
        "    i1, i2, l1, l2 = i1.to(device), i2.to(device), l1.to(device), l2.to(device)\n",
        "    i1 = i1.unsqueeze(1)  # Add a channel dimension\n",
        "    i2 = i2.unsqueeze(1)  # Add a channel dimension\n",
        "    output = siamese_net(i1, i2)\n",
        "    o1, o2 = output[0][0], output[0][1]\n",
        "    l = (l1 == l2).int()\n",
        "\n",
        "\n",
        "    # Adjust the shapes to match the criterion\n",
        "    # outputs = outputs.view(-1)  # Flatten the outputs to a 1D tensor\n",
        "    # seg = seg.view(-1)  # Flatten the seg tensor to a 1D tensor\n",
        "    loss = criterion(o1, o2, l)\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if (i + 1) % 500 == 0:\n",
        "      print(f\"Epoch [{epoch + 1} / {epochs}], Step [{i + 1} / {total_step} Loss {loss.item()}]\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4NnXFUDzSWjp",
        "outputId": "8bd9a4f5-1999-4373-9d1b-9f3fe398f29b"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1 / 2], Step [500 / 4460 Loss 3.259337688632513e-07]\n",
            "Epoch [1 / 2], Step [1000 / 4460 Loss 3.283045589341782e-05]\n",
            "Epoch [1 / 2], Step [1500 / 4460 Loss 1.054049062076956e-05]\n",
            "Epoch [1 / 2], Step [2000 / 4460 Loss 1.234111095982371e-06]\n",
            "Epoch [1 / 2], Step [2500 / 4460 Loss 1.1719105259544449e-06]\n",
            "Epoch [1 / 2], Step [3000 / 4460 Loss 3.3835010526672704e-07]\n",
            "Epoch [1 / 2], Step [3500 / 4460 Loss 7.921468636595819e-08]\n",
            "Epoch [1 / 2], Step [4000 / 4460 Loss 2.9854402328055585e-07]\n",
            "Epoch [2 / 2], Step [500 / 4460 Loss 1.767582347156349e-07]\n",
            "Epoch [2 / 2], Step [1000 / 4460 Loss 4.3589712395153413e-10]\n",
            "Epoch [2 / 2], Step [1500 / 4460 Loss 8.744237902647001e-07]\n",
            "Epoch [2 / 2], Step [2000 / 4460 Loss 2.0802056258095725e-11]\n",
            "Epoch [2 / 2], Step [2500 / 4460 Loss 1.8013805913597025e-07]\n",
            "Epoch [2 / 2], Step [3000 / 4460 Loss 3.1853244308877038e-06]\n",
            "Epoch [2 / 2], Step [3500 / 4460 Loss 2.546377402268263e-07]\n",
            "Epoch [2 / 2], Step [4000 / 4460 Loss 2.075099914122802e-08]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation loop\n",
        "siamese_net.eval()  # Set the model to evaluation mode\n",
        "with torch.no_grad():  # Disable gradient calculation for evaluation\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for i, (i1, i2, l1, l2) in enumerate(loaders['test']):\n",
        "        i1, i2, l1, l2 = i1.to(device), i2.to(device), l1.to(device), l2.to(device)\n",
        "        i1 = i1.unsqueeze(1)  # Add a channel dimension\n",
        "        i2 = i2.unsqueeze(1)  # Add a channel dimension\n",
        "        output = siamese_net(i1, i2)\n",
        "        o1, o2 = output[0][0], output[0][1]\n",
        "        l = (l1 == l2).int()\n",
        "        # Compute the Euclidean distance between the outputs\n",
        "        distance = torch.abs(o1 - o2)\n",
        "\n",
        "        # Apply threshold to determine predicted labels\n",
        "        threshold = 0.5  # Adjust as needed\n",
        "        predicted = distance.lt(threshold)\n",
        "\n",
        "\n",
        "\n",
        "        total += l\n",
        "        correct += (predicted == l).sum().item()\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "    print(f'Test Accuracy: {accuracy:.2f}%')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G3TWNpvKdAyn",
        "outputId": "40daccb3-3e72-4373-eebb-64c60fd8df2a"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: nan%\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyPMy7bIZ9steqmxN0P4lni6",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}