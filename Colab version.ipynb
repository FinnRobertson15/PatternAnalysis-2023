{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FinnRobertson15/PatternAnalysis-2023/blob/topic-recognition/Colab%20version.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yzCmT9TUJY10",
        "outputId": "f4a92c6b-c770-48bf-a03c-3a6b9f924c30"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\", force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Lb7CGdEJCQg",
        "outputId": "7cf9c217-28e6-43e3-aee4-f079779419c3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/root\n"
          ]
        }
      ],
      "source": [
        "%cd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "# Define the paths\n",
        "zip_path = '/content/drive/MyDrive/AD_NC.zip'  # Path to the zip file\n",
        "extract_path = '/content/extracted_folder'  # Path where you want to extract the contents\n",
        "\n",
        "# Unzip the file\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_path)\n",
        "\n",
        "# List the contents of the extracted folder\n",
        "extracted_files = os.listdir(extract_path)\n",
        "print(extracted_files)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PM8uTdbZpQGJ",
        "outputId": "15ea444a-ce03-4ddb-d19e-5a8bb8686b53"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['AD_NC']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd /content/extracted_folder/AD_NC"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x1t1iFV_p2MU",
        "outputId": "9d142efb-edd0-4ae2-8e69-1a377fb13ad5"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/extracted_folder/AD_NC\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tp_Fb7Eyp-yv",
        "outputId": "45e2140b-5eab-4cf6-dc81-3b6c9fd592f0"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0m\u001b[01;34mtest\u001b[0m/  \u001b[01;34mtrain\u001b[0m/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "_8PBaJLSJSCP"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as transforms\n",
        "import time\n",
        "import numpy as np\n",
        "# import pandas as pd\n",
        "import math\n",
        "import os\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.utils import shuffle\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.datasets import ImageFolder\n",
        "import os\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader, TensorDataset, ConcatDataset\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 128"
      ],
      "metadata": {
        "id": "u6EH0wk_CkxF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7zvKyWj2J7Yk"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, root_dir, transform=None):\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "        self.image_paths = os.listdir(root_dir)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = os.path.join(self.root_dir, self.image_paths[idx])\n",
        "        image = Image.open(img_name)\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image\n",
        "\n",
        "class SiameseDataset(Dataset):\n",
        "    def __init__(self, AD, NC):\n",
        "        # Combine the datasets and labels\n",
        "        self.X = AD + NC\n",
        "\n",
        "        self.Y = torch.cat((torch.ones(len(AD)), torch.zeros(len(NC))), dim=0)\n",
        "\n",
        "        # Generate a random permutation of indices\n",
        "        self.i_indices = torch.randperm(len(self.X) // 2)\n",
        "        self.j_indices = torch.randperm(len(self.X) // 2)\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X) // 2\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        i = self.i_indices[idx] * 2\n",
        "        j = self.j_indices[idx] * 2 + 1\n",
        "        img1 = self.X[i]\n",
        "        img2 = self.X[j]\n",
        "        l1 = self.Y[i]\n",
        "        l2 = self.Y[j]\n",
        "\n",
        "        return img1, img2, l1, l2\n",
        "\n",
        "# Set the device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "\n",
        "size = 64\n",
        "\n",
        "def intensity_normalization(img):\n",
        "    mean = torch.mean(img)\n",
        "    std = torch.std(img)\n",
        "    return (img - mean) / std\n",
        "\n",
        "def windowing(img, window_center, window_width):\n",
        "    img = torch.clamp(img, window_center - window_width // 2, window_center + window_width // 2)\n",
        "    img = (img - (window_center - 0.5)) / (window_width - 1)\n",
        "    return img\n",
        "\n",
        "# Example usage in your transform\n",
        "transform_X = transforms.Compose([\n",
        "    transforms.Resize((size, size)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Lambda(intensity_normalization),\n",
        "    transforms.Lambda(lambda x: windowing(x, window_center=size // 2, window_width=size))\n",
        "])\n",
        "\n",
        "\n",
        "# Replace 'your_nii_folder' with the path to your folder containing .nii files\n",
        "# nii_dataset = NiiDataset(root_dir=r'keras_png_slices_data\\keras_png_slices_data\\keras_png_slices_train')\n",
        "loaders = {}\n",
        "\n",
        "for stage in ['test']:\n",
        "    loaders[stage] = {}\n",
        "    AD = CustomDataset(root_dir=os.path.join(stage, 'AD'), transform=transform_X)\n",
        "    NC = CustomDataset(root_dir=os.path.join(stage, 'NC'), transform=transform_X)\n",
        "    dataset = SiameseDataset(AD, NC)\n",
        "    loaders[stage] = DataLoader(dataset, batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "count = 0\n",
        "same = 0\n",
        "for i, j, n, m in loaders['test']:\n",
        "  count += len(n)\n",
        "  same += (n == m).sum().tolist()\n",
        "same / count"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ceQcKMAPoip",
        "outputId": "2b8eb6b1-c1c5-43db-9370-42f0da1531f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5144444444444445"
            ]
          },
          "metadata": {},
          "execution_count": 225
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "puuj1CSURmJ9"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "\n",
        "class SiameseNetwork(nn.Module):\n",
        "    def __init__(self, pretrained=True):\n",
        "        super(SiameseNetwork, self).__init__()\n",
        "        self.resnet = models.resnet18(pretrained=False)\n",
        "        # Modify the first convolution layer to accept single-channel input\n",
        "        self.resnet.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "        self.fc1 = nn.Linear(1000, 500)  # Customize the fully connected layers\n",
        "        self.fc2 = nn.Linear(500, 2)  # Customize the fully connected layers\n",
        "\n",
        "    def forward(self, x1, x2):\n",
        "        output1 = self.resnet(x1)\n",
        "        output2 = self.resnet(x2)\n",
        "        output = torch.abs(output1 - output2)\n",
        "        output = self.fc1(output)\n",
        "        output = self.fc2(output)\n",
        "        return output\n",
        "\n",
        "model = SiameseNetwork()\n",
        "\n",
        "class ContrastiveLoss(torch.nn.Module):\n",
        "    def __init__(self, margin=2.0):\n",
        "        super(ContrastiveLoss, self).__init__()\n",
        "        self.margin = margin\n",
        "\n",
        "    def forward(self, output1, output2, label):\n",
        "        euclidean_distance = F.pairwise_distance(output1, output2)\n",
        "        loss_contrastive = torch.mean((1 - label) * torch.pow(euclidean_distance, 2) +\n",
        "                                      label * torch.pow(torch.clamp(self.margin - euclidean_distance, min=0.0), 2))\n",
        "        return loss_contrastive"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the network, loss function, and optimizer\n",
        "siamese_net = SiameseNetwork(pretrained=True).to(device)\n",
        "criterion = ContrastiveLoss()\n",
        "optimizer = optim.Adam(siamese_net.parameters(), lr=0.001)\n",
        "# Training loop\n",
        "epochs = 5\n",
        "total_step = len(loaders['test'])\n",
        "for epoch in range(epochs):\n",
        "  for i, (img1, img2, lab1, lab2) in enumerate(loaders['test']):\n",
        "    img1, img2, lab1, lab2 = img1.to(device), img2.to(device), lab1.to(device), lab2.to(device)\n",
        "    output = siamese_net(img1, img2)\n",
        "    o1, o2 = output[:, 0], output[:, 1]\n",
        "    label = (lab1 == lab2).int()\n",
        "\n",
        "    loss = criterion(o1, o2, label)\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if (i + 1) % 21 == 0:\n",
        "      print((lab1[0], lab2[0]))\n",
        "      print(label[0])\n",
        "      print(output[0])\n",
        "      print(f\"Epoch [{epoch + 1} / {epochs}], Step [{i + 1} / {total_step} Loss {loss.item()}]\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4NnXFUDzSWjp",
        "outputId": "ab9065a9-ffad-4405-963d-85ae26293b0d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(tensor(0., device='cuda:0'), tensor(1., device='cuda:0'))\n",
            "tensor(0, device='cuda:0', dtype=torch.int32)\n",
            "tensor([ 0.0997, -0.0306], device='cuda:0', grad_fn=<SelectBackward0>)\n",
            "Epoch [1 / 5], Step [21 / 141 Loss 0.9753031730651855]\n",
            "(tensor(0., device='cuda:0'), tensor(0., device='cuda:0'))\n",
            "tensor(1, device='cuda:0', dtype=torch.int32)\n",
            "tensor([ 0.1081, -0.0379], device='cuda:0', grad_fn=<SelectBackward0>)\n",
            "Epoch [1 / 5], Step [42 / 141 Loss 1.025275707244873]\n",
            "(tensor(1., device='cuda:0'), tensor(0., device='cuda:0'))\n",
            "tensor(0, device='cuda:0', dtype=torch.int32)\n",
            "tensor([ 0.0929, -0.0268], device='cuda:0', grad_fn=<SelectBackward0>)\n",
            "Epoch [1 / 5], Step [63 / 141 Loss 0.9390543699264526]\n",
            "(tensor(1., device='cuda:0'), tensor(0., device='cuda:0'))\n",
            "tensor(0, device='cuda:0', dtype=torch.int32)\n",
            "tensor([ 0.1265, -0.0541], device='cuda:0', grad_fn=<SelectBackward0>)\n",
            "Epoch [1 / 5], Step [84 / 141 Loss 1.0038050413131714]\n",
            "(tensor(1., device='cuda:0'), tensor(0., device='cuda:0'))\n",
            "tensor(0, device='cuda:0', dtype=torch.int32)\n",
            "tensor([ 0.0915, -0.0274], device='cuda:0', grad_fn=<SelectBackward0>)\n",
            "Epoch [1 / 5], Step [105 / 141 Loss 1.1157597303390503]\n",
            "(tensor(0., device='cuda:0'), tensor(1., device='cuda:0'))\n",
            "tensor(0, device='cuda:0', dtype=torch.int32)\n",
            "tensor([ 0.1292, -0.0573], device='cuda:0', grad_fn=<SelectBackward0>)\n",
            "Epoch [1 / 5], Step [126 / 141 Loss 1.0057425498962402]\n",
            "(tensor(0., device='cuda:0'), tensor(1., device='cuda:0'))\n",
            "tensor(0, device='cuda:0', dtype=torch.int32)\n",
            "tensor([ 0.0953, -0.0308], device='cuda:0', grad_fn=<SelectBackward0>)\n",
            "Epoch [2 / 5], Step [21 / 141 Loss 0.9697328805923462]\n",
            "(tensor(0., device='cuda:0'), tensor(0., device='cuda:0'))\n",
            "tensor(1, device='cuda:0', dtype=torch.int32)\n",
            "tensor([ 0.1175, -0.0509], device='cuda:0', grad_fn=<SelectBackward0>)\n",
            "Epoch [2 / 5], Step [42 / 141 Loss 1.004995584487915]\n",
            "(tensor(1., device='cuda:0'), tensor(0., device='cuda:0'))\n",
            "tensor(0, device='cuda:0', dtype=torch.int32)\n",
            "tensor([ 0.0923, -0.0304], device='cuda:0', grad_fn=<SelectBackward0>)\n",
            "Epoch [2 / 5], Step [63 / 141 Loss 0.9395542144775391]\n",
            "(tensor(1., device='cuda:0'), tensor(0., device='cuda:0'))\n",
            "tensor(0, device='cuda:0', dtype=torch.int32)\n",
            "tensor([ 0.1254, -0.0604], device='cuda:0', grad_fn=<SelectBackward0>)\n",
            "Epoch [2 / 5], Step [84 / 141 Loss 0.9549553394317627]\n",
            "(tensor(1., device='cuda:0'), tensor(0., device='cuda:0'))\n",
            "tensor(0, device='cuda:0', dtype=torch.int32)\n",
            "tensor([ 0.0939, -0.0335], device='cuda:0', grad_fn=<SelectBackward0>)\n",
            "Epoch [2 / 5], Step [105 / 141 Loss 1.1070666313171387]\n",
            "(tensor(0., device='cuda:0'), tensor(1., device='cuda:0'))\n",
            "tensor(0, device='cuda:0', dtype=torch.int32)\n",
            "tensor([ 0.1300, -0.0632], device='cuda:0', grad_fn=<SelectBackward0>)\n",
            "Epoch [2 / 5], Step [126 / 141 Loss 0.9884251952171326]\n",
            "(tensor(0., device='cuda:0'), tensor(1., device='cuda:0'))\n",
            "tensor(0, device='cuda:0', dtype=torch.int32)\n",
            "tensor([ 0.0944, -0.0337], device='cuda:0', grad_fn=<SelectBackward0>)\n",
            "Epoch [3 / 5], Step [21 / 141 Loss 0.9663693308830261]\n",
            "(tensor(0., device='cuda:0'), tensor(0., device='cuda:0'))\n",
            "tensor(1, device='cuda:0', dtype=torch.int32)\n",
            "tensor([ 0.1245, -0.0624], device='cuda:0', grad_fn=<SelectBackward0>)\n",
            "Epoch [3 / 5], Step [42 / 141 Loss 1.0025595426559448]\n",
            "(tensor(1., device='cuda:0'), tensor(0., device='cuda:0'))\n",
            "tensor(0, device='cuda:0', dtype=torch.int32)\n",
            "tensor([ 0.1142, -0.0528], device='cuda:0', grad_fn=<SelectBackward0>)\n",
            "Epoch [3 / 5], Step [63 / 141 Loss 0.9942120313644409]\n",
            "(tensor(1., device='cuda:0'), tensor(0., device='cuda:0'))\n",
            "tensor(0, device='cuda:0', dtype=torch.int32)\n",
            "tensor([ 0.1249, -0.0629], device='cuda:0', grad_fn=<SelectBackward0>)\n",
            "Epoch [3 / 5], Step [84 / 141 Loss 0.9485695362091064]\n",
            "(tensor(1., device='cuda:0'), tensor(0., device='cuda:0'))\n",
            "tensor(0, device='cuda:0', dtype=torch.int32)\n",
            "tensor([ 0.0966, -0.0371], device='cuda:0', grad_fn=<SelectBackward0>)\n",
            "Epoch [3 / 5], Step [105 / 141 Loss 1.1140365600585938]\n",
            "(tensor(0., device='cuda:0'), tensor(1., device='cuda:0'))\n",
            "tensor(0, device='cuda:0', dtype=torch.int32)\n",
            "tensor([ 0.1298, -0.0664], device='cuda:0', grad_fn=<SelectBackward0>)\n",
            "Epoch [3 / 5], Step [126 / 141 Loss 0.9871245622634888]\n",
            "(tensor(0., device='cuda:0'), tensor(1., device='cuda:0'))\n",
            "tensor(0, device='cuda:0', dtype=torch.int32)\n",
            "tensor([ 0.0954, -0.0360], device='cuda:0', grad_fn=<SelectBackward0>)\n",
            "Epoch [4 / 5], Step [21 / 141 Loss 0.96551513671875]\n",
            "(tensor(0., device='cuda:0'), tensor(0., device='cuda:0'))\n",
            "tensor(1, device='cuda:0', dtype=torch.int32)\n",
            "tensor([ 0.1227, -0.0626], device='cuda:0', grad_fn=<SelectBackward0>)\n",
            "Epoch [4 / 5], Step [42 / 141 Loss 1.0030264854431152]\n",
            "(tensor(1., device='cuda:0'), tensor(0., device='cuda:0'))\n",
            "tensor(0, device='cuda:0', dtype=torch.int32)\n",
            "tensor([ 0.1154, -0.0552], device='cuda:0', grad_fn=<SelectBackward0>)\n",
            "Epoch [4 / 5], Step [63 / 141 Loss 0.9911506175994873]\n",
            "(tensor(1., device='cuda:0'), tensor(0., device='cuda:0'))\n",
            "tensor(0, device='cuda:0', dtype=torch.int32)\n",
            "tensor([ 0.1292, -0.0673], device='cuda:0', grad_fn=<SelectBackward0>)\n",
            "Epoch [4 / 5], Step [84 / 141 Loss 0.9427274465560913]\n",
            "(tensor(1., device='cuda:0'), tensor(0., device='cuda:0'))\n",
            "tensor(0, device='cuda:0', dtype=torch.int32)\n",
            "tensor([ 0.0964, -0.0375], device='cuda:0', grad_fn=<SelectBackward0>)\n",
            "Epoch [4 / 5], Step [105 / 141 Loss 1.1125668287277222]\n",
            "(tensor(0., device='cuda:0'), tensor(1., device='cuda:0'))\n",
            "tensor(0, device='cuda:0', dtype=torch.int32)\n",
            "tensor([ 0.1283, -0.0657], device='cuda:0', grad_fn=<SelectBackward0>)\n",
            "Epoch [4 / 5], Step [126 / 141 Loss 0.9870107173919678]\n",
            "(tensor(0., device='cuda:0'), tensor(1., device='cuda:0'))\n",
            "tensor(0, device='cuda:0', dtype=torch.int32)\n",
            "tensor([ 0.0937, -0.0353], device='cuda:0', grad_fn=<SelectBackward0>)\n",
            "Epoch [5 / 5], Step [21 / 141 Loss 0.9660702347755432]\n",
            "(tensor(0., device='cuda:0'), tensor(0., device='cuda:0'))\n",
            "tensor(1, device='cuda:0', dtype=torch.int32)\n",
            "tensor([ 0.1197, -0.0604], device='cuda:0', grad_fn=<SelectBackward0>)\n",
            "Epoch [5 / 5], Step [42 / 141 Loss 1.0011703968048096]\n",
            "(tensor(1., device='cuda:0'), tensor(0., device='cuda:0'))\n",
            "tensor(0, device='cuda:0', dtype=torch.int32)\n",
            "tensor([ 0.1150, -0.0551], device='cuda:0', grad_fn=<SelectBackward0>)\n",
            "Epoch [5 / 5], Step [63 / 141 Loss 0.9866605997085571]\n",
            "(tensor(1., device='cuda:0'), tensor(0., device='cuda:0'))\n",
            "tensor(0, device='cuda:0', dtype=torch.int32)\n",
            "tensor([ 0.1265, -0.0649], device='cuda:0', grad_fn=<SelectBackward0>)\n",
            "Epoch [5 / 5], Step [84 / 141 Loss 0.9338618516921997]\n",
            "(tensor(1., device='cuda:0'), tensor(0., device='cuda:0'))\n",
            "tensor(0, device='cuda:0', dtype=torch.int32)\n",
            "tensor([ 0.0969, -0.0381], device='cuda:0', grad_fn=<SelectBackward0>)\n",
            "Epoch [5 / 5], Step [105 / 141 Loss 1.1113743782043457]\n",
            "(tensor(0., device='cuda:0'), tensor(1., device='cuda:0'))\n",
            "tensor(0, device='cuda:0', dtype=torch.int32)\n",
            "tensor([ 0.1214, -0.0599], device='cuda:0', grad_fn=<SelectBackward0>)\n",
            "Epoch [5 / 5], Step [126 / 141 Loss 0.9903947114944458]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "correct = 0\n",
        "total = 0\n",
        "features = []\n",
        "labels = []\n",
        "with torch.no_grad():\n",
        "    for i, (img1, img2, lab1, lab2) in enumerate(loaders['test']):\n",
        "        img1, img2, lab1, lab2 = img1.to(device), img2.to(device), lab1.to(device), lab2.to(device)\n",
        "        output = siamese_net(img1, img2)\n",
        "        out1, out2 = output[:, 0], output[:, 1]\n",
        "        predicted = (out1 - out2).pow(2).sum().sqrt().lt(0.5)\n",
        "        total += lab1.size(0)\n",
        "        correct += (predicted == lab1).sum().item()\n",
        "        features.extend(out1.tolist())\n",
        "        labels.extend(lab1.tolist())\n",
        "        features.extend(out2.tolist())\n",
        "        labels.extend(lab2.tolist())\n",
        "\n",
        "\n",
        "    print(f'Accuracy of the network on the test images: {100 * correct / total}%')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G3TWNpvKdAyn",
        "outputId": "bf70c5fd-38e5-49c7-a64f-ba1758f103d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the network on the test images: 49.955555555555556%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(np.array(features).reshape(-1, 1), labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train a logistic regression model\n",
        "classifier = LogisticRegression(max_iter=1000)\n",
        "classifier.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "predictions = classifier.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, predictions)\n",
        "print(f\"Accuracy: {accuracy}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MVEvstik8zqK",
        "outputId": "c5aed97c-27c1-4685-bf6a-cc2204dbaa69"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.49166666666666664\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qA-60lBMGV3i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "features[0].tolist()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9PrC3ijsFnhE",
        "outputId": "42aa07d0-9166-43fb-a67e-713fc99664da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5943959355354309"
            ]
          },
          "metadata": {},
          "execution_count": 148
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hqYJrbkVfi36"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "batch_size = 128\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, root_dir, transform=None):\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "        self.image_paths = os.listdir(root_dir)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = os.path.join(self.root_dir, self.image_paths[idx])\n",
        "        image = Image.open(img_name)\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image\n",
        "\n",
        "class TripletDataset(Dataset):\n",
        "    def __init__(self, AD, NC):\n",
        "        # Combine the datasets and labels\n",
        "        self.X = AD + NC\n",
        "        self.AD = AD\n",
        "        self.NC = NC\n",
        "\n",
        "        self.Y = torch.cat((torch.ones(len(AD)), torch.zeros(len(NC))), dim=0)\n",
        "\n",
        "        # Generate random permutations of indices\n",
        "        self.anc_indices = torch.randperm(len(self.X))\n",
        "        self.pos_indices = self.anc_indices % len(AD)\n",
        "        self.neg_indices = self.anc_indices % len(NC)\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.anc_indices)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        anc = self.anc_indices[idx]\n",
        "        pos = self.pos_indices[idx]\n",
        "        neg = self.neg_indices[idx]\n",
        "        img1 = self.X[anc]\n",
        "        img2 = self.AD[pos]\n",
        "        img3 = self.NC[neg]\n",
        "        label = self.Y[anc]\n",
        "\n",
        "        return img1, img2, img3, label\n",
        "\n",
        "\n",
        "# Set the device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "\n",
        "size = 256\n",
        "\n",
        "def intensity_normalization(img):\n",
        "    mean = torch.mean(img)\n",
        "    std = torch.std(img)\n",
        "    return (img - mean) / std\n",
        "\n",
        "def windowing(img, window_center, window_width):\n",
        "    img = torch.clamp(img, window_center - window_width // 2, window_center + window_width // 2)\n",
        "    img = (img - (window_center - 0.5)) / (window_width - 1)\n",
        "    return img\n",
        "\n",
        "transform_t = transforms.Compose([\n",
        "    transforms.Resize((size, size)),\n",
        "    # transforms.RandomHorizontalFlip(p=0.5),  # Randomly flip the image horizontally with 50% probability\n",
        "    # transforms.RandomVerticalFlip(p=0.5),  # Randomly flip the image vertically with 50% probability\n",
        "    transforms.RandomRotation(degrees=15),  # Randomly rotate the image by up to 5 degrees\n",
        "    transforms.ColorJitter(brightness=0.05, contrast=0.05, saturation=0.05, hue=0.05),  # Adjust color jitter with smaller increments\n",
        "    transforms.RandomAffine(degrees=0, translate=(0.05, 0.05), scale=(0.95, 1.05)),  # Random affine transformations with smaller parameters\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Lambda(intensity_normalization),\n",
        "    transforms.Lambda(lambda x: windowing(x, window_center=size // 2, window_width=size))\n",
        "])\n",
        "\n",
        "transform_X = transforms.Compose([\n",
        "    transforms.Resize((size, size)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Lambda(intensity_normalization),\n",
        "    transforms.Lambda(lambda x: windowing(x, window_center=size // 2, window_width=size))\n",
        "])\n",
        "\n",
        "\n",
        "# Replace 'your_nii_folder' with the path to your folder containing .nii files\n",
        "# nii_dataset = NiiDataset(root_dir=r'keras_png_slices_data\\keras_png_slices_data\\keras_png_slices_train')\n",
        "loaders = {}\n",
        "\n",
        "for stage in ['train', 'test']:\n",
        "    transform = transform_t if stage == 'train' else transform_X\n",
        "    AD = CustomDataset(root_dir=os.path.join(stage, 'AD'), transform=transform)\n",
        "    NC = CustomDataset(root_dir=os.path.join(stage, 'NC'), transform=transform)\n",
        "    dataset = TripletDataset(AD, NC)\n",
        "    loaders[stage] = DataLoader(dataset, batch_size=batch_size)"
      ],
      "metadata": {
        "id": "dlrwhZTjTKWf"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class TripletSiameseNetwork(nn.Module):\n",
        "    def __init__(self, pretrained=True):\n",
        "        super(TripletSiameseNetwork, self).__init__()\n",
        "        self.resnet = models.resnet18(pretrained=pretrained)\n",
        "        self.resnet.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "        self.fc1 = nn.Linear(1000, 500)\n",
        "        self.fc2 = nn.Linear(500, 20)\n",
        "\n",
        "    def forward_once(self, x):\n",
        "        output = self.resnet(x)\n",
        "        output = self.fc1(output)\n",
        "        output = self.fc2(output)\n",
        "        return output\n",
        "\n",
        "    def forward(self, anchor, positive, negative):\n",
        "        output_anchor = self.forward_once(anchor)\n",
        "        output_positive = self.forward_once(positive)\n",
        "        output_negative = self.forward_once(negative)\n",
        "        return output_anchor, output_positive, output_negative\n",
        "\n",
        "class TripletLoss(nn.Module):\n",
        "    def __init__(self, margin=1.0):\n",
        "        super(TripletLoss, self).__init__()\n",
        "        self.margin = margin\n",
        "\n",
        "    def forward(self, anchor, positive, negative):\n",
        "        distance_positive = F.pairwise_distance(anchor, positive)\n",
        "        distance_negative = F.pairwise_distance(anchor, negative)\n",
        "        losses = F.relu(distance_positive - distance_negative + self.margin)\n",
        "        return losses.mean()\n",
        "\n",
        "class TripletLossWithRegularization(nn.Module):\n",
        "    def __init__(self, margin=1.0, lambda_reg=0.01):\n",
        "        super(TripletLossWithRegularization, self).__init__()\n",
        "        self.margin = margin\n",
        "        self.lambda_reg = lambda_reg  # Regularization parameter\n",
        "\n",
        "    def forward(self, anchor, positive, negative):\n",
        "        distance_positive = F.pairwise_distance(anchor, positive)\n",
        "        distance_negative = F.pairwise_distance(anchor, negative)\n",
        "        triplet_losses = F.relu(distance_positive - distance_negative + self.margin)\n",
        "        triplet_loss = triplet_losses.mean()\n",
        "\n",
        "        # Compute L2 regularization\n",
        "        l2_reg = None\n",
        "        for param in trip_model.parameters():\n",
        "            if l2_reg is None:\n",
        "                l2_reg = param.norm(2)\n",
        "            else:\n",
        "                l2_reg = l2_reg + param.norm(2)\n",
        "\n",
        "        loss = triplet_loss + self.lambda_reg * l2_reg\n",
        "        return loss\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "c8MhXUlOTHnY"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.optim.lr_scheduler import StepLR\n",
        "learning_rate = 0.0005\n",
        "trip_model = TripletSiameseNetwork()\n",
        "trip_criterion = TripletLossWithRegularization(margin=1.0)\n",
        "total_step = len(loaders['train'])\n",
        "\n",
        "optimizer = optim.Adam(trip_model.parameters(), lr=learning_rate)\n",
        "scheduler = StepLR(optimizer, step_size=3, gamma=0.1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gs0V-oZe1O46",
        "outputId": "adaef8b1-fced-4a59-88e3-e7a58373c6ae"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
            "100%|██████████| 44.7M/44.7M [00:00<00:00, 186MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from itertools import cycle\n",
        "\n",
        "test_iter = cycle(iter(loaders['test']))\n",
        "# Training loop\n",
        "epochs = 10\n",
        "trip_model.to(device)\n",
        "losses = []\n",
        "val_losses = []\n",
        "for epoch in range(epochs):\n",
        "  losses.append([])\n",
        "  val_losses.append([])\n",
        "  for i, (img1, img2, img3, _) in enumerate(loaders['train']):\n",
        "    img1, img2, img3 = img1.to(device), img2.to(device), img3.to(device)\n",
        "    size = img1.size(0)\n",
        "    img1, img2, img3 = img1[torch.randperm(size)], img2[torch.randperm(size)], img3[torch.randperm(size)]\n",
        "    out1, out2, out3 = trip_model(img1, img2, img3)\n",
        "\n",
        "    loss = trip_criterion(out1, out2, out3)\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if (i + 1) % (total_step // 10) == 0:\n",
        "      losses[epoch].append(loss.item())\n",
        "\n",
        "      val_img1, val_img2, val_img3, _ = next(test_iter)\n",
        "      val_img1, val_img2, val_img3 = val_img1.to(device), val_img2.to(device), val_img3.to(device)\n",
        "      val_out1, val_out2, val_out3 = trip_model(val_img1, val_img2, val_img3)\n",
        "      val_loss = trip_criterion(val_out1, val_out2, val_out3)\n",
        "      val_losses[epoch].append(val_loss.item())\n",
        "\n",
        "      print(f\"Epoch [{epoch + 1} / {epochs}], Step [{i + 1} / {total_step}], Loss: {loss.item()}, Validation Loss: {val_loss.item()}\")\n",
        "\n",
        "    scheduler.step()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "id": "A_J3lXLOUDu0",
        "outputId": "380a9cc5-fffd-4b7a-ea04-2ac87e26aab3"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1 / 10], Step [16 / 169], Loss: 6.299764156341553, Validation Loss: 5.821728706359863\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "OutOfMemoryError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-32827f37bc51>\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0msize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mimg1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandperm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandperm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg3\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandperm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mout1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrip_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrip_criterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-9c5959dba5ad>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, anchor, positive, negative)\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manchor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpositive\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnegative\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0moutput_anchor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manchor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0moutput_positive\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpositive\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0moutput_negative\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnegative\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput_anchor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_positive\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_negative\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-9c5959dba5ad>\u001b[0m in \u001b[0;36mforward_once\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/models/resnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    283\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 285\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/models/resnet.py\u001b[0m in \u001b[0;36m_forward_impl\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    267\u001b[0m         \u001b[0;31m# See note [TorchScript super()]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 269\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    270\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaxpool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/batchnorm.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    169\u001b[0m         \u001b[0mused\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mnormalization\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0;32min\u001b[0m \u001b[0meval\u001b[0m \u001b[0mmode\u001b[0m \u001b[0mwhen\u001b[0m \u001b[0mbuffers\u001b[0m \u001b[0mare\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m         \"\"\"\n\u001b[0;32m--> 171\u001b[0;31m         return F.batch_norm(\n\u001b[0m\u001b[1;32m    172\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m             \u001b[0;31m# If buffers are not to be tracked, ensure that they won't be updated\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mbatch_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[1;32m   2448\u001b[0m         \u001b[0m_verify_batch_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2449\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2450\u001b[0;31m     return torch.batch_norm(\n\u001b[0m\u001b[1;32m   2451\u001b[0m         \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunning_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunning_var\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcudnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2452\u001b[0m     )\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 512.00 MiB (GPU 0; 15.77 GiB total capacity; 14.33 GiB already allocated; 28.12 MiB free; 14.47 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings = []\n",
        "labels = []\n",
        "trip_model.eval()\n",
        "with torch.no_grad():\n",
        "    for i, (img1, img2, img3, label) in enumerate(loaders['test']):\n",
        "        img1, img2, img3, label = img1.to(device), img2.to(device), img3.to(device), label.to(device)\n",
        "        out1, _, _ = trip_model(img1, img2, img3)\n",
        "        embeddings.extend(out1.cpu().tolist())\n",
        "        labels.extend(label.cpu().tolist())\n"
      ],
      "metadata": {
        "id": "nnGoOwX-Vk_i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(embeddings, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train a logistic regression model\n",
        "classifier = LogisticRegression(max_iter=1000)\n",
        "classifier.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "predictions = classifier.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, predictions)\n",
        "print(f\"Accuracy: {accuracy}\")\n"
      ],
      "metadata": {
        "id": "ayGTRCP5Wl0s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gaj_LIDgrl6N"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyPKn/v2mVLUWZMauFxXGvWg",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}